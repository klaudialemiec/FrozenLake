{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake \n",
    "This notebook presents how to play FrozenLake game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import mlflow\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_run import train, play\n",
    "from src.models import PPO_Agent, A2C_Agent, DQN_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'file///{(Path(\"../reports\") / \"mlruns\").resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'PPO_deterministic' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"PPO_deterministic\"\n",
    "experiment_path = Path(\"../reports\") / experiment_name\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\", is_slippery=False)\n",
    "eval_env = gym.make(\"FrozenLake8x8-v0\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO_Agent(MlpPolicy, env, tensorboard_log=experiment_path/\"tensorboard/\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PROJEKTY\\GAT\\gat-ml-eng-devtest\\gym_env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 14.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    train(model=agent, timesteps=5000000, eval_env=eval_env, model_path=Path(\"../models/\") / experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "agent = PPO_Agent.load(Path(\"../models/PPO_deterministic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFH\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(agent, eval_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'PPO_stochastic' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"PPO_stochastic\"\n",
    "experiment_path = Path(\"../reports\") / experiment_name\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "eval_env = gym.make(\"FrozenLake8x8-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO_Agent(MlpPolicy, env, tensorboard_log=experiment_path/\"tensorboard/\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 15.80 +/- 15.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 37.50 +/- 30.66\n",
      "Eval num_timesteps=30000, episode_reward=0.10 +/- 0.30\n",
      "Episode length: 53.70 +/- 40.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.30 +/- 0.46\n",
      "Episode length: 51.20 +/- 38.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=0.30 +/- 0.46\n",
      "Episode length: 42.20 +/- 21.52\n",
      "Eval num_timesteps=60000, episode_reward=0.90 +/- 0.30\n",
      "Episode length: 80.90 +/- 45.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 81.20 +/- 48.33\n",
      "Eval num_timesteps=80000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 102.90 +/- 56.62\n",
      "Eval num_timesteps=90000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 101.10 +/- 38.57\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    train(model=agent, timesteps=5000000, eval_env=eval_env, model_path=Path(\"../models/\") / experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'A2C_stochastic' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"A2C_stochastic\"\n",
    "experiment_path = Path(\"../reports\") / experiment_name\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "eval_env = gym.make(\"FrozenLake8x8-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C_Agent(MlpPolicy, env, tensorboard_log=experiment_path/\"tensorboard/\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PROJEKTY\\GAT\\gat-ml-eng-devtest\\gym_env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 22.50 +/- 11.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.30 +/- 0.46\n",
      "Episode length: 65.80 +/- 63.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 40.10 +/- 52.16\n",
      "Eval num_timesteps=40000, episode_reward=0.30 +/- 0.46\n",
      "Episode length: 42.90 +/- 57.66\n",
      "Eval num_timesteps=50000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 68.50 +/- 45.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.90 +/- 0.30\n",
      "Episode length: 101.20 +/- 43.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 106.30 +/- 50.15\n",
      "Eval num_timesteps=80000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 72.80 +/- 43.83\n",
      "Eval num_timesteps=90000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 122.00 +/- 55.66\n",
      "Eval num_timesteps=100000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 101.50 +/- 29.94\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    train(model=agent, timesteps=5000000, eval_env=eval_env, model_path=Path(\"../models/\") / experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"DQN_stochastic\"\n",
    "experiment_path = Path(\"../reports\") / experiment_name\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "eval_env = gym.make(\"FrozenLake8x8-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_Agent(\"MlpPolicy\", env, tensorboard_log=experiment_path/\"tensorboard/\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PROJEKTY\\GAT\\gat-ml-eng-devtest\\gym_env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 58.70 +/- 47.07\n",
      "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 161.40 +/- 61.23\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 92.90 +/- 65.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 115.10 +/- 63.85\n",
      "Eval num_timesteps=110000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 136.50 +/- 57.04\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 165.80 +/- 57.92\n",
      "Eval num_timesteps=130000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 109.40 +/- 75.06\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=0.10 +/- 0.30\n",
      "Episode length: 96.60 +/- 63.09\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=0.40 +/- 0.49\n",
      "Episode length: 80.40 +/- 43.75\n",
      "Eval num_timesteps=180000, episode_reward=0.10 +/- 0.30\n",
      "Episode length: 86.60 +/- 77.77\n",
      "Eval num_timesteps=190000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 173.10 +/- 53.84\n",
      "Eval num_timesteps=200000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 129.70 +/- 59.02\n",
      "Eval num_timesteps=210000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 59.70 +/- 44.18\n",
      "Eval num_timesteps=220000, episode_reward=0.30 +/- 0.46\n",
      "Episode length: 173.50 +/- 44.64\n",
      "Eval num_timesteps=230000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 119.10 +/- 56.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=0.10 +/- 0.30\n",
      "Episode length: 134.60 +/- 66.75\n",
      "Eval num_timesteps=250000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 105.30 +/- 34.17\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    train(model=agent, timesteps=5000000, eval_env=eval_env, model_path=Path(\"../models/\") / experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "gym_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
